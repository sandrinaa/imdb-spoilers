{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJJMdL7zfgUp",
        "outputId": "36aa3957-2d64-4e8e-9a13-2b6bd48d6b41"
      },
      "outputs": [],
      "source": [
        "#%pip install contractions\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nest_asyncio\n",
        "import asyncio\n",
        "import re\n",
        "import html\n",
        "import contractions\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from textblob import TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [],
      "source": [
        "# read data\n",
        "movie_details = pd.read_json('IMDB_movie_details.json', lines=True)\n",
        "reviews_details = pd.read_json('IMDB_reviews.json', lines=True)\n",
        "\n",
        "reviews = pd.merge(reviews_details, movie_details, on = 'movie_id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/qw/0q1k53t510l2k5m76b5xnqp40000gn/T/ipykernel_4268/2759537044.py:15: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
            "  return pd.to_datetime(s, errors='coerce', dayfirst=dayfirst)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unparsed release_date: 0\n",
            "Empty DataFrame\n",
            "Columns: [release_date]\n",
            "Index: []\n"
          ]
        }
      ],
      "source": [
        "def parse_partial_dates(s, dayfirst=True):\n",
        "    s = s.astype('string').str.strip()\n",
        "\n",
        "    # YYYY -> YYYY-01-01\n",
        "    s = s.where(~s.str.fullmatch(r'\\d{4}'), s + '-01-01')\n",
        "\n",
        "    # YYYY-MM -> YYYY-MM-01\n",
        "    s = s.where(~s.str.fullmatch(r'\\d{4}-(0[1-9]|1[0-2])'), s + '-01')\n",
        "    \n",
        "    # clean up spaces and commas\n",
        "    s = (s.str.replace(r'\\s+', ' ', regex=True)\n",
        "           .str.replace(',', '', regex=False))\n",
        "\n",
        "    # parse (handles things like \"1 July 2000\" -> \"1957-04-01\")\n",
        "    return pd.to_datetime(s, errors='coerce', dayfirst=dayfirst)\n",
        "\n",
        "reviews['release_date'] = parse_partial_dates(reviews['release_date'])\n",
        "reviews['review_date']  = parse_partial_dates(reviews['review_date'])\n",
        "\n",
        "# extract year, month, day into new columns\n",
        "for col in ['review_date','release_date']:\n",
        "    base = col.split('_')[0]     # 'review' / 'release'\n",
        "    reviews[f'{base}_year']  = reviews[col].dt.year\n",
        "    reviews[f'{base}_month'] = reviews[col].dt.month\n",
        "    reviews[f'{base}_day']   = reviews[col].dt.day\n",
        "\n",
        "# inspect anything still missing\n",
        "print(\"Unparsed release_date:\", reviews['release_date'].isna().sum())\n",
        "print(reviews[reviews['release_date'].isna()][['release_date']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [],
      "source": [
        "# extract year, month, day into new columns\n",
        "reviews['review_year']  = reviews['review_date'].dt.year\n",
        "reviews['review_month'] = reviews['review_date'].dt.month\n",
        "reviews['review_day']   = reviews['review_date'].dt.day\n",
        "\n",
        "reviews['release_year']  = reviews['release_date'].dt.year\n",
        "reviews['release_month'] = reviews['release_date'].dt.month\n",
        "reviews['release_day']   = reviews['release_date'].dt.day\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [],
      "source": [
        "# normalize duration to minutes\n",
        "reviews['duration'] = (\n",
        "    reviews['duration'].str.extract(r'(\\d+)h\\s*(\\d+)?')\n",
        "    .fillna(0)\n",
        "    .astype(int)\n",
        "    .apply(lambda x: x[0]*60 + x[1], axis=1)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n"
          ]
        }
      ],
      "source": [
        "print(reviews.isna().any(axis=1).sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [],
      "source": [
        "reviews[\"polarity\"] = reviews[\"review_text\"].apply(lambda s: TextBlob(s).sentiment.polarity)\n",
        "reviews[\"subjectivity\"] = reviews[\"review_text\"].apply(lambda s: TextBlob(s).sentiment.subjectivity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXE9R1EuE_XY",
        "outputId": "b9fe575e-009f-489f-9ee9-ff1cb687e993"
      },
      "outputs": [],
      "source": [
        "# nltk.download('punkt_tab')\n",
        "# nltk.download('wordnet')\n",
        "\n",
        "# splitting the data into training and testing sets\n",
        "reviews_train, reviews_test = train_test_split(reviews, test_size=0.2, random_state=3244)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "5mXlS1gBUbJb"
      },
      "outputs": [],
      "source": [
        "# apply nest_asyncio to enable nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# custom stop-words: removed words with negative connotations, he/she/they/them, modal verbs, intensity/polarity words, explanatory words\n",
        "custom_stop_words = ['about', 'above', 'after', 'an', 'and', 'any', 'as', 'be', 'been', 'before', 'being', 'below',\n",
        "                    'between', 'both', 'by', 'does', 'doing', 'down', 'during', 'each', 'few', 'from', 'further',\n",
        "                    'had', 'has', 'having', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his',   \"i'd\", 'if',\n",
        "                    \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn',  'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\",\n",
        "                    'me',  'more', 'most', 'myself',  'nor', 'now', 'of', 'off', 'on', 'once', 'only', 'or', 'other',\n",
        "                    'our', 'ours', 'ourselves', 'out', 'over', 'own', 'same',  'some', 'such', 'than', 'that',\n",
        "                    \"that'll\",  'their', 'theirs', 'then', 'there', 'these',  'this', 'those', 'through', 'under',\n",
        "                    'until', 'up', 'was', 'we', \"we'd\", \"we'll\", \"we're\", 'were',  \"we've\", 'which', 'while', 'who',\n",
        "                    'whom',  \"you'd\", \"you'll\", \"you're\", 'yours', 'yourself', 'yourselves', \"you've\", \"film\", \"movie\",\n",
        "                    \"character\", \"story\", \"show\", \"time\", \"make\", \"see\", \"think\", \"even\", \"way\", \"one\", \"will\", \"much\",\n",
        "                    \"really\", \"good\", \"bad\", \"well\", \"people\", \"great\", \"work\", \"watch\", \"look\", \"better\", \"take\",\n",
        "                    \"love\", \"life\", \"actor\", \"performance\", \"scene\", \"director\", \"world\", \"feel\", \"first\", \"know\",\n",
        "                    \"little\", \"still\", \"want\", \"thing\", \"going\", \"part\", \"end\", \"made\", \"lot\", \"man\", \"quite\", \"never\",\n",
        "                    'actually', 'maybe', 'though', 'always', 'find', 'fun']\n",
        "\n",
        "# given that there are no null values in the dataset, we only check for duplicates\n",
        "def duplicates(data):\n",
        "    cols = [c for c in data.columns if c != 'genre']\n",
        "    data.drop_duplicates(subset=cols, inplace=True)\n",
        "    return data\n",
        "\n",
        "reviews_train = duplicates(reviews_train)\n",
        "\n",
        "# remove HTML characters\n",
        "def r_html(text):\n",
        "    return html.unescape(text)\n",
        "\n",
        "# replace URLs with word 'URL'\n",
        "def urls(text):\n",
        "    return re.sub(r'https?://[A-Za-z0-9./]+', 'url', text)\n",
        "\n",
        "# drop duplicate sentences in a text\n",
        "def duplicate_sentences(text):\n",
        "    sentences = text.split('.')\n",
        "    sentences = list(dict.fromkeys(sentences))\n",
        "    return '.'.join(sentences)\n",
        "\n",
        "# lowercase the text\n",
        "def lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "# expand contractions\n",
        "def r_contractions(text):\n",
        "    return contractions.fix(text)\n",
        "\n",
        "# remove special characters\n",
        "def special_characters(text):\n",
        "    return re.sub(r'[^a-zA-Z@\\s]', ' ', text)\n",
        "\n",
        "# replace 3 or more consecutive letters with 2 letters\n",
        "def consecutive_letters(text):\n",
        "    return re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
        "\n",
        "# Function to remove custom stopwords and join back the words\n",
        "def custom_stopwords(text):\n",
        "    words = word_tokenize(text)\n",
        "    text = [word for word in words if word not in custom_stop_words]\n",
        "    return \" \".join(text)\n",
        "\n",
        "# Function to lemmatize the words (13)\n",
        "def lemmatize(text):\n",
        "    words = word_tokenize(text)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    text = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return \" \".join(text)\n",
        "\n",
        "# Define a function to pipeline the preprocessing steps with array to turn on and off the steps\n",
        "def preprocess_text(text, steps):\n",
        "    if 'html' in steps:\n",
        "        text = r_html(text)\n",
        "    if 'url' in steps:\n",
        "        text = urls(text)\n",
        "    if 'dupes' in steps:\n",
        "        text = duplicate_sentences(text)\n",
        "    if 'lower' in steps:\n",
        "        text = lowercase(text)\n",
        "    if 'expand' in steps:\n",
        "        text = r_contractions(text)\n",
        "    if 'special' in steps:\n",
        "        text = special_characters(text)\n",
        "    if 'replace3' in steps:\n",
        "        text = consecutive_letters(text)\n",
        "    if 'custom' in steps:\n",
        "        text = custom_stopwords(text)\n",
        "    if 'lemmatize' in steps:\n",
        "        text = lemmatize(text)\n",
        "    return text\n",
        "\n",
        "# define the preprocessing steps to be used in order\n",
        "steps = ['html', 'url', 'dupes', 'lower', 'expand', 'special', 'replace3', 'custom', 'lemmatize']\n",
        "\n",
        "# apply the preprocessing steps to the data\n",
        "reviews_train['review_text'] = reviews_train['review_text'].apply(lambda x: preprocess_text(x, steps))\n",
        "\n",
        "# drop NA\n",
        "reviews_train.replace({\"\": np.nan, None: np.nan}, inplace=True)\n",
        "reviews_train.dropna(subset=['review_text'], inplace=True)  # Drop rows with NaN in 'reviews' column, possible due to removal of entire sentences that contain only stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save train and test separately\n",
        "reviews_train.to_json('IMDB_reviews_train_cleaned.json', index=False)\n",
        "reviews_test.to_json('IMDB_reviews_test.json', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_date</th>\n",
              "      <th>movie_id</th>\n",
              "      <th>user_id</th>\n",
              "      <th>is_spoiler</th>\n",
              "      <th>review_text</th>\n",
              "      <th>rating_x</th>\n",
              "      <th>review_summary</th>\n",
              "      <th>plot_summary</th>\n",
              "      <th>duration</th>\n",
              "      <th>genre</th>\n",
              "      <th>...</th>\n",
              "      <th>release_date</th>\n",
              "      <th>plot_synopsis</th>\n",
              "      <th>review_year</th>\n",
              "      <th>review_month</th>\n",
              "      <th>review_day</th>\n",
              "      <th>release_year</th>\n",
              "      <th>release_month</th>\n",
              "      <th>release_day</th>\n",
              "      <th>polarity</th>\n",
              "      <th>subjectivity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>0 rows Ã— 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [review_date, movie_id, user_id, is_spoiler, review_text, rating_x, review_summary, plot_summary, duration, genre, rating_y, release_date, plot_synopsis, review_year, review_month, review_day, release_year, release_month, release_day, polarity, subjectivity]\n",
              "Index: []\n",
              "\n",
              "[0 rows x 21 columns]"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reviews_train[reviews_train[['review_year','review_month','review_day',\n",
        "                   'release_year','release_month','release_day']].isna().any(axis=1)]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "imdb310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
