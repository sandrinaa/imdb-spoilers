{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJJMdL7zfgUp",
        "outputId": "36aa3957-2d64-4e8e-9a13-2b6bd48d6b41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.12/dist-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.12/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.12/dist-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.12/dist-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install contractions\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import nest_asyncio\n",
        "import asyncio\n",
        "import re\n",
        "import html\n",
        "import contractions\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from keras.optimizers import Adam\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXE9R1EuE_XY",
        "outputId": "b9fe575e-009f-489f-9ee9-ff1cb687e993"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# read data\n",
        "reviews = pd.read_json('IMDB_reviews.json', lines=True)\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# splitting the data into training and testing sets\n",
        "reviews_train, reviews_test = train_test_split(reviews, test_size=0.2, random_state=3244)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mXlS1gBUbJb"
      },
      "outputs": [],
      "source": [
        "# apply nest_asyncio to enable nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# custom stop-words: removed words with negative connotations, he/she/they/them, modal verbs, intensity/polarity words, explanatory words\n",
        "custom_stop_words = ['about', 'above', 'after', 'an', 'and', 'any', 'as', 'be', 'been', 'before', 'being', 'below',\n",
        "                    'between', 'both', 'by', 'does', 'doing', 'down', 'during', 'each', 'few', 'from', 'further',\n",
        "                    'had', 'has', 'having', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his',   \"i'd\", 'if',\n",
        "                    \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn',  'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\",\n",
        "                    'me',  'more', 'most', 'myself',  'nor', 'now', 'of', 'off', 'on', 'once', 'only', 'or', 'other',\n",
        "                    'our', 'ours', 'ourselves', 'out', 'over', 'own', 'same',  'some', 'such', 'than', 'that',\n",
        "                    \"that'll\",  'their', 'theirs', 'then', 'there', 'these',  'this', 'those', 'through', 'under',\n",
        "                    'until', 'up', 'was', 'we', \"we'd\", \"we'll\", \"we're\", 'were',  \"we've\", 'which', 'while', 'who',\n",
        "                    'whom',  \"you'd\", \"you'll\", \"you're\", 'yours', 'yourself', 'yourselves', \"you've\", \"film\", \"movie\",\n",
        "                    \"character\", \"story\", \"show\", \"time\", \"make\", \"see\", \"think\", \"even\", \"way\", \"one\", \"will\", \"much\",\n",
        "                    \"really\", \"good\", \"bad\", \"well\", \"people\", \"great\", \"work\", \"watch\", \"look\", \"better\", \"take\",\n",
        "                    \"love\", \"life\", \"actor\", \"performance\", \"scene\", \"director\", \"world\", \"feel\", \"first\", \"know\",\n",
        "                    \"little\", \"still\", \"want\", \"thing\", \"going\", \"part\", \"end\", \"made\", \"lot\", \"man\", \"quite\", \"never\",\n",
        "                    'actually', 'maybe', 'though', 'always', 'find', 'fun']\n",
        "\n",
        "# given that there are no null values in the dataset, we only check for duplicates\n",
        "def duplicates(data):\n",
        "    data.drop_duplicates(inplace=True)\n",
        "    return data\n",
        "\n",
        "reviews_train = duplicates(reviews_train)\n",
        "\n",
        "# remove HTML characters\n",
        "def r_html(text):\n",
        "    return html.unescape(text)\n",
        "\n",
        "# replace URLs with word 'URL'\n",
        "def urls(text):\n",
        "    return re.sub(r'https?://[A-Za-z0-9./]+', 'url', text)\n",
        "\n",
        "# drop duplicate sentences in a text\n",
        "def duplicate_sentences(text):\n",
        "    sentences = text.split('.')\n",
        "    sentences = list(dict.fromkeys(sentences))\n",
        "    return '.'.join(sentences)\n",
        "\n",
        "# lowercase the text\n",
        "def lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "# expand contractions\n",
        "def r_contractions(text):\n",
        "    return contractions.fix(text)\n",
        "\n",
        "# remove special characters\n",
        "def special_characters(text):\n",
        "    return re.sub(r'[^a-zA-Z@\\s]', ' ', text)\n",
        "\n",
        "# replace 3 or more consecutive letters with 2 letters\n",
        "def consecutive_letters(text):\n",
        "    return re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
        "\n",
        "# Function to remove custom stopwords and join back the words\n",
        "def custom_stopwords(text):\n",
        "    words = word_tokenize(text)\n",
        "    text = [word for word in words if word not in custom_stop_words]\n",
        "    return \" \".join(text)\n",
        "\n",
        "# Function to lemmatize the words (13)\n",
        "def lemmatize(text):\n",
        "    words = word_tokenize(text)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    text = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return \" \".join(text)\n",
        "\n",
        "# Define a function to pipeline the preprocessing steps with array to turn on and off the steps\n",
        "def preprocess_text(text, steps):\n",
        "    if 'html' in steps:\n",
        "        text = r_html(text)\n",
        "    if 'url' in steps:\n",
        "        text = urls(text)\n",
        "    if 'dupes' in steps:\n",
        "        text = duplicate_sentences(text)\n",
        "    if 'lower' in steps:\n",
        "        text = lowercase(text)\n",
        "    if 'expand' in steps:\n",
        "        text = r_contractions(text)\n",
        "    if 'special' in steps:\n",
        "        text = special_characters(text)\n",
        "    if 'replace3' in steps:\n",
        "        text = consecutive_letters(text)\n",
        "    if 'custom' in steps:\n",
        "        text = custom_stopwords(text)\n",
        "    if 'lemmatize' in steps:\n",
        "        text = lemmatize(text)\n",
        "    return text\n",
        "\n",
        "# define the preprocessing steps to be used in order\n",
        "steps = ['html', 'url', 'dupes', 'lower', 'expand', 'special', 'replace3', 'custom', 'lemmatize']\n",
        "\n",
        "# apply the preprocessing steps to the data\n",
        "reviews_train['review_text'] = reviews_train['review_text'].apply(lambda x: preprocess_text(x, steps))\n",
        "\n",
        "# drop NA\n",
        "reviews_train.replace({\"\": np.nan, None: np.nan}, inplace=True)\n",
        "reviews_train.dropna(subset=['review_text'], inplace=True)  # Drop rows with NaN in 'reviews' column, possible due to removal of entire sentences that contain only stop words\n",
        "\n",
        "# save train and test separately\n",
        "reviews_train.to_json('data/processed/IMDB_reviews_train_cleaned.json', index=False)\n",
        "reviews_test.to_json('data/processed/IMDB_reviews_test.json', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
