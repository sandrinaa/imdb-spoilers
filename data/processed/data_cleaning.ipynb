{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJJMdL7zfgUp",
        "outputId": "36aa3957-2d64-4e8e-9a13-2b6bd48d6b41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: contractions in /Users/angelicagonathan/miniconda3/envs/imdb310/lib/python3.10/site-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /Users/angelicagonathan/miniconda3/envs/imdb310/lib/python3.10/site-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /Users/angelicagonathan/miniconda3/envs/imdb310/lib/python3.10/site-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
            "Requirement already satisfied: pyahocorasick in /Users/angelicagonathan/miniconda3/envs/imdb310/lib/python3.10/site-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install contractions\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import nest_asyncio\n",
        "import asyncio\n",
        "import re\n",
        "import html\n",
        "import contractions\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from keras.optimizers import Adam\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXE9R1EuE_XY",
        "outputId": "b9fe575e-009f-489f-9ee9-ff1cb687e993"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/angelicagonathan/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/angelicagonathan/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "# read data\n",
        "reviews = pd.read_json('IMDB_reviews.json', lines=True)\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# splitting the data into training and testing sets\n",
        "reviews_train, reviews_test = train_test_split(reviews, test_size=0.2, random_state=3244)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mXlS1gBUbJb"
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "Cannot save file into a non-existent directory: 'data/processed'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 103\u001b[0m\n\u001b[1;32m    100\u001b[0m reviews_train\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview_text\u001b[39m\u001b[38;5;124m'\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Drop rows with NaN in 'reviews' column, possible due to removal of entire sentences that contain only stop words\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# save train and test separately\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m \u001b[43mreviews_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_json\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/processed/IMDB_reviews_train_cleaned.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m reviews_test\u001b[38;5;241m.\u001b[39mto_json(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/processed/IMDB_reviews_test.json\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "File \u001b[0;32m~/miniconda3/envs/imdb310/lib/python3.10/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/imdb310/lib/python3.10/site-packages/pandas/core/generic.py:2724\u001b[0m, in \u001b[0;36mNDFrame.to_json\u001b[0;34m(self, path_or_buf, orient, date_format, double_precision, force_ascii, date_unit, default_handler, lines, compression, index, indent, storage_options, mode)\u001b[0m\n\u001b[1;32m   2721\u001b[0m config\u001b[38;5;241m.\u001b[39mis_nonnegative_int(indent)\n\u001b[1;32m   2722\u001b[0m indent \u001b[38;5;241m=\u001b[39m indent \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 2724\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_json\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2727\u001b[0m \u001b[43m    \u001b[49m\u001b[43morient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdouble_precision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdouble_precision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_ascii\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_ascii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault_handler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_handler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2739\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/imdb310/lib/python3.10/site-packages/pandas/io/json/_json.py:217\u001b[0m, in \u001b[0;36mto_json\u001b[0;34m(path_or_buf, obj, orient, date_format, double_precision, force_ascii, date_unit, default_handler, lines, compression, index, indent, storage_options, mode)\u001b[0m\n\u001b[1;32m    213\u001b[0m     s \u001b[38;5;241m=\u001b[39m convert_to_line_delimits(s)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path_or_buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    220\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle\u001b[38;5;241m.\u001b[39mwrite(s)\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[0;32m~/miniconda3/envs/imdb310/lib/python3.10/site-packages/pandas/io/common.py:749\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[0;32m--> 749\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/imdb310/lib/python3.10/site-packages/pandas/io/common.py:616\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    614\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m--> 616\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'data/processed'"
          ]
        }
      ],
      "source": [
        "# apply nest_asyncio to enable nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# custom stop-words: removed words with negative connotations, he/she/they/them, modal verbs, intensity/polarity words, explanatory words\n",
        "custom_stop_words = ['about', 'above', 'after', 'an', 'and', 'any', 'as', 'be', 'been', 'before', 'being', 'below',\n",
        "                    'between', 'both', 'by', 'does', 'doing', 'down', 'during', 'each', 'few', 'from', 'further',\n",
        "                    'had', 'has', 'having', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his',   \"i'd\", 'if',\n",
        "                    \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn',  'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\",\n",
        "                    'me',  'more', 'most', 'myself',  'nor', 'now', 'of', 'off', 'on', 'once', 'only', 'or', 'other',\n",
        "                    'our', 'ours', 'ourselves', 'out', 'over', 'own', 'same',  'some', 'such', 'than', 'that',\n",
        "                    \"that'll\",  'their', 'theirs', 'then', 'there', 'these',  'this', 'those', 'through', 'under',\n",
        "                    'until', 'up', 'was', 'we', \"we'd\", \"we'll\", \"we're\", 'were',  \"we've\", 'which', 'while', 'who',\n",
        "                    'whom',  \"you'd\", \"you'll\", \"you're\", 'yours', 'yourself', 'yourselves', \"you've\", \"film\", \"movie\",\n",
        "                    \"character\", \"story\", \"show\", \"time\", \"make\", \"see\", \"think\", \"even\", \"way\", \"one\", \"will\", \"much\",\n",
        "                    \"really\", \"good\", \"bad\", \"well\", \"people\", \"great\", \"work\", \"watch\", \"look\", \"better\", \"take\",\n",
        "                    \"love\", \"life\", \"actor\", \"performance\", \"scene\", \"director\", \"world\", \"feel\", \"first\", \"know\",\n",
        "                    \"little\", \"still\", \"want\", \"thing\", \"going\", \"part\", \"end\", \"made\", \"lot\", \"man\", \"quite\", \"never\",\n",
        "                    'actually', 'maybe', 'though', 'always', 'find', 'fun']\n",
        "\n",
        "# given that there are no null values in the dataset, we only check for duplicates\n",
        "def duplicates(data):\n",
        "    data.drop_duplicates(inplace=True)\n",
        "    return data\n",
        "\n",
        "reviews_train = duplicates(reviews_train)\n",
        "\n",
        "# remove HTML characters\n",
        "def r_html(text):\n",
        "    return html.unescape(text)\n",
        "\n",
        "# replace URLs with word 'URL'\n",
        "def urls(text):\n",
        "    return re.sub(r'https?://[A-Za-z0-9./]+', 'url', text)\n",
        "\n",
        "# drop duplicate sentences in a text\n",
        "def duplicate_sentences(text):\n",
        "    sentences = text.split('.')\n",
        "    sentences = list(dict.fromkeys(sentences))\n",
        "    return '.'.join(sentences)\n",
        "\n",
        "# lowercase the text\n",
        "def lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "# expand contractions\n",
        "def r_contractions(text):\n",
        "    return contractions.fix(text)\n",
        "\n",
        "# remove special characters\n",
        "def special_characters(text):\n",
        "    return re.sub(r'[^a-zA-Z@\\s]', ' ', text)\n",
        "\n",
        "# replace 3 or more consecutive letters with 2 letters\n",
        "def consecutive_letters(text):\n",
        "    return re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
        "\n",
        "# Function to remove custom stopwords and join back the words\n",
        "def custom_stopwords(text):\n",
        "    words = word_tokenize(text)\n",
        "    text = [word for word in words if word not in custom_stop_words]\n",
        "    return \" \".join(text)\n",
        "\n",
        "# Function to lemmatize the words (13)\n",
        "def lemmatize(text):\n",
        "    words = word_tokenize(text)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    text = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return \" \".join(text)\n",
        "\n",
        "# Define a function to pipeline the preprocessing steps with array to turn on and off the steps\n",
        "def preprocess_text(text, steps):\n",
        "    if 'html' in steps:\n",
        "        text = r_html(text)\n",
        "    if 'url' in steps:\n",
        "        text = urls(text)\n",
        "    if 'dupes' in steps:\n",
        "        text = duplicate_sentences(text)\n",
        "    if 'lower' in steps:\n",
        "        text = lowercase(text)\n",
        "    if 'expand' in steps:\n",
        "        text = r_contractions(text)\n",
        "    if 'special' in steps:\n",
        "        text = special_characters(text)\n",
        "    if 'replace3' in steps:\n",
        "        text = consecutive_letters(text)\n",
        "    if 'custom' in steps:\n",
        "        text = custom_stopwords(text)\n",
        "    if 'lemmatize' in steps:\n",
        "        text = lemmatize(text)\n",
        "    return text\n",
        "\n",
        "# define the preprocessing steps to be used in order\n",
        "steps = ['html', 'url', 'dupes', 'lower', 'expand', 'special', 'replace3', 'custom', 'lemmatize']\n",
        "\n",
        "# apply the preprocessing steps to the data\n",
        "reviews_train['review_text'] = reviews_train['review_text'].apply(lambda x: preprocess_text(x, steps))\n",
        "\n",
        "# drop NA\n",
        "reviews_train.replace({\"\": np.nan, None: np.nan}, inplace=True)\n",
        "reviews_train.dropna(subset=['review_text'], inplace=True)  # Drop rows with NaN in 'reviews' column, possible due to removal of entire sentences that contain only stop words\n",
        "\n",
        "# save train and test separately\n",
        "reviews_train.to_json('IMDB_reviews_train_cleaned.json', index=False)\n",
        "reviews_test.to_json('IMDB_reviews_test.json', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "imdb310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
